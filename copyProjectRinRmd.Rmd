---
title: "Untitled"
author: "Eva Salivonik"
date: "Tuesday, October 18th 2023"
output:
  html_document:
    toc: yes
    toc_depth: '5'
    df_print: paged
  pdf_document:
    fig_height: 4
    fig_width: 5
    number_sections: yes
    toc: yes
    toc_depth: 5
editor_options: 
  markdown: 
    wrap: 72
    always_allow_html: true
---

```{r message=FALSE, warning=FALSE}

```

```{r setup, include=FALSE}
library(tidyverse)
library(scales)
library(gridExtra)
library(corrplot)
library(caret)   #for general data preparation and model fitting
library(plotly)
library(plyr)
library(ggplot2)
library(gdata)
library(xgboost)   #for fitting the xgboost model
library(prettydoc)
library(magrittr)
library(RColorBrewer)
library(ROSE)
library(pROC)
library(ggplot2)     # to plot
library(gridExtra)   # to put more
library(grid)  
library(tinytex)
library(latexpdf)
library(Ckmeans.1d.dp)
knitr::opts_chunk$set(echo = FALSE)
```
```{r}
library(latex2exp)

library(tinytex)
```
# Business Introduction

## Abstract
Cross-selling is a sales strategy employed to encourage customers to increase their spending by acquiring a product that complements their current purchase. This approach finds utility in the insurance industry, where it serves as a means for companies to introduce new products to their existing customer base. Leveraging machine learning models can streamline the labor-intensive process of sifting through customer records, resulting in significant time and cost savings. However, the implementation of machine learning models poses its own set of challenges, some of which we aim to address in this project. Recognizing the pivotal role of data in business, we prioritize customer privacy, adhering to the principles of the Ethical Machine Learning framework. Our team conducts data analysis and model development following the CRISP-DM methodology.


## Background 
Triks Insurance Inc., a prominent Life Insurance Agent in the town of XYZ, boasts an extensive client base of 381,109 households within the region. Presently, they are in negotiations with a prominent Auto Insurance provider, GOOL Auto Insurance, with plans to introduce their insurance product. GOOL Auto Insurance has requested ABC Insurance to furnish a report indicating the proportion of their client base likely to express interest in Auto insurance.

Rather than embarking on the arduous task of manually sifting through their customer database, Triks Insurance Inc. has opted to engage the SK team to create a machine learning system. This system's purpose is not only to predict potential Auto Insurance candidates once but continuously over time. By comprehending the customer base and leveraging demographic information about a prospect, this machine learning system will assess whether a prospect falls into the category of a "good" or "bad" candidate.

By implementing this solution, Triks Insurance's sales team will be better prepared for their cross-selling endeavors. They can hone their focus on the most promising leads, thus maximizing the efficiency of their marketing budget while concentrating on the most lucrative prospects.





The proposed solution serves several valuable purposes:

*It provides GOOL Auto with insights into the potential business that can be generated from Triks Insurance's existing client base.

*It equips the staff at Triks Insurance with a tool for client prioritization, facilitating focused and effective marketing campaigns.

*It enables Triks Insurance to project future revenues, enhancing their financial planning.

*The system offers real-time notifications for potential cross-selling opportunities.

The client was initially unaware that the data they possess holds the answers to their questions. Some of their concerns and inquiries include:

*What information or resources do you require from us to deliver this solution?

*Can the system accurately classify customers?

*How will the system handle cases where a customer who should have qualified for GOOL Insurance is not selected?

## Objective 
The aim of this project is to create one or more models to generate an initial report for GOOL Insurance on behalf of Triks Insurance Ltd. Additionally, the project seeks to develop a predictive tool that can determine whether a customer, based on specific characteristics, qualifies as a potential candidate for their upcoming cross-selling campaign.

## Business Understanding:
To understand and address business problems:

*Determine the key variables to predict (good/bad candidates).
*Define relevant metrics related to these variables.
*Grasp project objectives and requirements.
*Create specific questions, such as what distinguishes a good from a bad prospect.
*Assess current sales practices for identifying prospects.
*Establish a success metric for the project.
*Identify data sources with answers to these questions.
*Select data that accurately measures the target and relevant features.
*Consider if the existing system requires additional data for the project.
*Evaluate the need for external data sources or system updates.

Then, translate this knowledge into a data mining problem and create a preliminary plan to achieve the project goals.

## Data Understanding:

Having understood the business statement, it's clear that ABC Insurance faced a choice. They could have promoted Auto Insurance to all their customers, but that wouldn't have been the most efficient use of their marketing budget. It's more cost-effective to focus on customers likely to respond to the Aplus Auto campaign. This targeted approach saves money and doesn't bother customers uninterested in the new product.

To achieve this, we can leverage historical data from past campaigns to build a model for predicting which customers are promising prospects. We start by collecting relevant data, addressing data quality issues, gaining initial insights from the data, and identifying interesting trends or patterns within it.

The data for this project was downloaded from Kaggle, weblink: [https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction](https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction).
For privacy and security, the customer names have been masked with an id.The data is in csv format.


## Data Description :

| Variable Name          | Variable Description                               |
| :-------------------  | :------------------------------------------------ |
|id                      |Unique ID for the customer                          | 
|Gender                  |Gender of the customer                              |
|Age                     |Age of the customer                                 |
|Driving_License         |0 : Customer does not have DL                       |
|                        |1 : Customer already has DL                         |
|Region_Code             |Unique code for the region of the customer          |
|Previously_Insured      |1 : Customer already has Vehicle Insurance.         |
|                        |0 : Customer doesn't have Vehicle Insurance.        |
|Vehicle_Age             |Age of the Vehicle                                  |
|Vehicle_Damage          |1 : Customer's vehicle damaged in the past.         |
|                        |0 : Customer"s vehicle no damaged in the past.      |
|Annual_Premium          |The premium insurec paid in the year                |
|Policy_Sales_Channel    |Anonymized Code for outreach channels for sales.    |
|Vintage                 |Number of Days, Customer has been associated with.  |
|                        |the company                                         |
|Response                |1 : Customer is interested                          |
|                        |0 : Customer is not interested                      |



## Loading Data

```{r}

test <- read.csv("c://DATA//test.csv")
train <- read.csv('c://DATA//train.csv')
submission <- read.csv('c://DATA//sample_submission.csv')
```

## Helper Functions ??

```{r}
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }
```

##Determine the dimension of our dataset:

```{r}
dim(train)
```

## View the contents and structure of our dataset:

```{r}
head(train)

head(test)
```

## View the main structure of the raw data

```{r}
str(train)
```

### EDA ??

We can create a profile of the ideal customer for our insurance company:

The ideal customer doesn't currently have active insurance.
They have a history of vehicle accidents or damage.
Their vehicle is relatively new, between 1 to 2 years old.
They fall within the age range of 30 to 55.
They pay an annual premium between 30,000 and 40,000.
They are more inclined to use channels like channel 25 or channel 125, rather than channel 155.
## Customer Response

```{r}
a1 <- train %>% mutate(c_resp = factor(Response)) %>%
  mutate(c_resp = revalue(c_resp,c("0"= "Not Interested","1"="Interested"))) %>%
  ggplot(aes(c_resp,fill=c_resp)) + 
  geom_bar(color='black') +
  scale_y_continuous(labels=ks) +
  labs(title='Customer Response',x='Customer response') + 
  theme(legend.title=element_blank()) +
  scale_fill_manual(values= c('#3C3838','#338076'),name='') +
  theme_classic() 

fig1 <- ggplotly(a1)
fig1
#
```

## What affect customer response?

# Response by previous insurance

```{r}
a2 <- train %>% mutate(c_resp=factor(Response),Insured=factor(Previously_Insured)) %>%
  mutate(c_resp=revalue(c_resp,c("0"='Not Interested',"1"='Interested')),Insured=revalue(Insured,c("0"='Not Insured',"1"='Insured'))) %>%
  ggplot(aes(c_resp,fill=Insured)) + 
  geom_bar(position='fill',color='black') + 
  scale_y_continuous(labels=percent) + 
  theme(legend.title = element_blank()) + 
  labs(x='Customer response',title='Response by previous insurances') +
  scale_fill_manual(values= c('#3C3838','#338076')) +
  theme_classic()
```

# Response by past vehicle damage

```{r}
a3 <- train %>% mutate(c_resp=factor(Response)) %>%
  mutate(c_resp= revalue(c_resp,c("0"="Not Interested","1"="Interested"))) %>%
  ggplot(aes(c_resp,fill=Vehicle_Damage)) +
  geom_bar(color="black",position='fill') +
  scale_y_continuous(labels=percent) + 
  labs(x='Customer response',title='Response by vehicle history') +
  scale_fill_manual(values= c('#3C3838','#338076'),name="Damaged?") +
  theme_classic()
```

# Response by gender

```{r}
a4 <- train %>% mutate(c_resp=factor(Response)) %>%
  mutate(c_resp= revalue(c_resp,c("0"="Not Interested","1"="Interested"))) %>%
  ggplot(aes(c_resp,fill=Gender)) +
  geom_bar(color="black",position='fill') +
  scale_y_continuous(labels=percent) + 
  labs(x='Customer response',title='Response by gender') +
  scale_fill_manual(values= c('#3C3838','#338076'),name="Gender") +
  theme_classic()
```

# Response by vehicle age

```{r}
a5 <- train %>% mutate(c_resp=factor(Response)) %>%
  mutate(c_resp= revalue(c_resp,c("0"="Not Interested","1"="Interested"))) %>%
  ggplot(aes(c_resp,fill=Vehicle_Age)) +
  geom_bar(color="black",position='fill') +
  scale_y_continuous(labels=percent) + 
  labs(x='Customer response',title='Response by vehicle age') +
  scale_fill_manual(values= c('#3C3838','#338076',"#333B80"),name='Vehicle age') +
  theme_classic()

grid.arrange(a1,a3,a4,a5)

```

## Response by age

```{r}
a6 <- train %>% mutate(c_resp=factor(Response)) %>%
  mutate(c_resp= revalue(c_resp,c("0"="Not Interested","1"="Interested"))) %>%
  ggplot(aes(Age,fill=c_resp)) +
  geom_density(alpha=0.7) +
  labs(title='Customer response by age') +
  scale_y_continuous(labels=percent) +
  scale_fill_manual(values= c('#3C3838','#338076'),name='Response')+
  theme_classic()
fig2 <- ggplotly(a6)
fig2
```

## Response by Annual Premium

```{r}
a7 =  train %>% mutate(c_resp=factor(Response)) %>%
  mutate(c_resp= revalue(c_resp,c("0"="Not Interested","1"="Interested"))) %>%
  ggplot(aes(Annual_Premium,fill=c_resp)) +
  geom_histogram(color='black',bins=50) +
  labs(title='Customer response by annual premium') +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 25),labels=ks) +
  scale_y_continuous(labels=ks) +
  scale_fill_manual(values= c('#3C3838','#338076'),name='Response') +
  theme_classic() +
  theme(legend.position="top",axis.text.x = element_text(size =8 ,angle = 30)) 


fig3 = ggplotly(a7)
fig3

```

## What Affect vehicle damage?

# Age by Vehicle Damage

```{r}
b1 <- train %>% ggplot(aes(Age,fill=Vehicle_Damage)) + 
  geom_density(alpha=0.7) + labs(title='Who had a vehicle damaged?') + 
  scale_y_continuous(labels=percent) + 
  scale_fill_manual(values= c('#3C3838','#338076'),name="Damaged?") +
  theme_classic()
```

# Vehicle damage by vehicle age

```{r}
b2<- train %>% ggplot(aes(Vehicle_Damage,fill=Vehicle_Age)) + 
  geom_bar(position='fill',color='black') + 
  scale_y_continuous(labels=percent) + 
  labs(title='Vehicle damaged by age') +
  scale_fill_manual(values= c('#3C3838','#338076',"#333B80"),name="Vehicle age") +
  theme_classic()
```

# vehicle damage by gender

```{r}
b3 <- train %>% ggplot(aes(Vehicle_Damage,fill=Gender)) + 
  geom_bar(position = 'fill',color='black') + 
  scale_y_continuous(labels=percent) + 
  labs(title='Vehicle damage by gender') +
  scale_fill_manual(values= c('#3C3838','#338076')) +
  theme_classic()

grid.arrange(b1,b2,b3,layout_matrix=rbind(c(1),c(2,3)))
```

## Annual Premium

# annual premium by vehicle damage

```{r}
c1 <- train %>% ggplot(aes(Annual_Premium,fill=Vehicle_Damage)) + 
  geom_histogram(color='black',bins=30) + 
  scale_x_continuous(breaks=scales::pretty_breaks(n =30),labels=ks) +
  scale_y_continuous(labels=ks) +
  labs(title='Annual Premium based on past vehicle damages') + 
  scale_fill_manual(values= c('#3C3838','#338076'),name='Damaged?') +
  theme_classic() +
  theme(legend.position="top",axis.text.x = element_text(size =10 ,angle = 30))

c2 <- train %>% ggplot(aes(Annual_Premium,fill=Vehicle_Age)) + 
  geom_histogram(color='white',bins=) + 
  scale_x_continuous(breaks=scales::pretty_breaks(n = 30),labels=ks) +
  scale_y_continuous(labels=ks) +
  labs(title='Annual Premium based on Vehicle Age') + 
  scale_fill_manual(values= c('#3C3838','#338076',"#333B80"),name="Vehicle age")+
  theme_classic() +
  theme(legend.position="top",axis.text.x = element_text(size =10 ,angle = 30))

c3 <- train %>% mutate(insured=factor(Previously_Insured)) %>% 
  mutate(insured=revalue(insured,c("0"="Not Insured","1"="Insured"))) %>% 
  ggplot(aes(Annual_Premium,fill=insured)) +
  geom_histogram(color='black') + 
  scale_x_continuous(breaks=scales::pretty_breaks(n = 30),labels=ks) +
  scale_y_continuous(labels=ks) +
  labs(title='Annual Premium based on Vehicle Age') + 
  scale_fill_manual(values= c('#338076','#3C3838'),name="Status")+
  theme_classic() +
  theme(legend.position="top",axis.text.x = element_text(size =10 ,angle = 30))



grid.arrange(c1,c2)

c3
```

## Age distribution by Vehicle Age

```{r}
d3 <- train %>% 
  ggplot(aes(Age,fill=Vehicle_Age)) + 
  geom_density(alpha=0.8) + 
  labs(title='Age distribution by Vehicle Age') + 
  scale_x_continuous(breaks=scales::pretty_breaks(n = 22)) + 
  scale_fill_manual(values=c('#3C3838','#338076',"#333B80")) + 
  theme_classic()

fig10 <- ggplotly(d3)

fig10
```

## Policy sales channel

```{r}
train %>% mutate(c_resp=factor(Response)) %>%
  mutate(c_resp= revalue(c_resp,c("0"="Not Interested","1"="Interested"))) %>%
  ggplot(aes(Policy_Sales_Channel,fill=c_resp),) + 
  geom_density(alpha=0.5,color="white") + 
  labs(title='Response by sales channel') + 
  scale_x_continuous(breaks=scales::pretty_breaks(n = 22)) + 
  scale_fill_manual(values=c('#3C3838','#338076'),name='Response') +
  theme_classic()

train %>% mutate(insured=factor(Previously_Insured)) %>% mutate(insured=revalue(insured,c("0"="Not Insured","1"="Insured"))) %>% 
  ggplot(aes(Policy_Sales_Channel,fill=insured)) + 
  geom_density(alpha=0.5,color="white") + 
  labs(title='Distribution of sales channel by Insured/Not insured customers') + 
  scale_x_continuous(breaks=scales::pretty_breaks(n = 22)) + 
  scale_fill_manual(values=c('#3C3838','#338076'),name='Have insurance?') +
  theme_classic()

train  %>% ggplot(aes(Policy_Sales_Channel,fill=Vehicle_Age)) + 
  geom_density(alpha=0.5,color="white") + 
  labs(title='Distribution of sales channel by Vehicle age') + 
  scale_x_continuous(breaks=scales::pretty_breaks(n = 22)) + 
  scale_fill_manual(values=c('#3C3838','#338076','red'),name='Vehicle age') +
  theme_classic()

train  %>% ggplot(aes(Policy_Sales_Channel,fill=Vehicle_Damage)) + 
  geom_density(alpha=0.5,color="white") + 
  labs(title='Distribution of sales channel by Vehicle Damage') + 
  scale_x_continuous(breaks=scales::pretty_breaks(n = 22)) + 
  scale_fill_manual(values=c('#3C3838','#338076','red'),name='Damaged?') +
  theme_classic()

```

# Correlation

Examine the relationships between independent numerical variables by creating a correlation matrix to assess the correlation coefficients between these variables. Before constructing the correlation matrix, exclude the 'id' and 'Response' columns.


```{r}
numeric_cols = sapply(train, is.numeric)
train_num_only= train[, numeric_cols]

train_num_only$id = NULL
train_num_only$Response = NULL
```

#Correlation matrix 1 is good, all charachters are removed, since
correlation can be perform on numeric only

```{r}
cor_result = cor(as.matrix(train_num_only))

train_corr <- train_num_only %>% mutate_if(is.numeric, list(scale))
cor(train_corr, use="pairwise.complete.obs", method = "spearman")   %>%  
  
  corrplot( method = "pie", outline = T, addgrid.col = "darkgray", mar = c(0.1,0,0.1,0), order = 'AOE', type = "full",rect.col = "black", rect.lwd = 5, cl.pos = "b", tl.col = "black", tl.cex = 1, cl.cex =.5,tl.srt=50,col=c('#3C3838',"gray",'#338076'),number.cex = 7/ncol(train_corr))
```

#to check

```{r}
class(train_corr)
head(train_corr)
```



# The XGBoost model, a boosting technique in machine learning, is known for its ability to generate highly accurate predictive models.

#**How deal with am imbalanced dataset? 
There are numerous approaches, and while many of them are valid, this time we decided to take a different path.


# How we proceed:

Our approach is as follows:

*We'll fine-tune certain XGBoost parameters using grid search.
*We'll apply the XGBoost Classifier to the imbalanced dataset (the original dataset), while setting the scale_pos_weight : sum(negative)/sum(positive).
*We'll assess the XGBoost Classifier's performance on a balanced dataset that matches the length of the training dataset.
*Finally, we'll submit the results. 

Since all customers in the submission dataset are uninterested, we will focus solely on evaluating the model's metrics.


# We will tune some XGB parameters with gridsearch.


## XGB Gridsearch

#Oversampling for gridsearch

```{r}
ov_sampling <- ovun.sample(formula = Response ~. ,data=train,method='both',N=100,seed=12)
train_2<- ov_sampling$data
```

# GRID SEARCH

```{r}
xgb_gridsearch <- function(train,ntrees) 
{
  set.seed(1)
  print("Best hyperparameters combination: ")
  
  # One hot encoding
  dmy <- dummyVars(" ~ .", data = train)
  train_data <- data.frame(predict(dmy, newdata = train))
  
  # as.numeric
  for (i in ncol(train_data)){
    train_data[[i]] = as.numeric(train_data[[i]])
  }
  
  # Factor Response for grid search
  grid_train = train_data
  grid_train$Response = factor(grid_train$Response)
  levels(grid_train$Response) <- c("X0","X1")
  
  ntrees <- 10
  
  # parameters grid
  xgb_grid_1 = expand.grid(
    nrounds = ntrees,
    eta = seq(2,10,by=1)/ntrees,
    max_depth = c(6, 8, 10),
    gamma = 0,
    subsample = c(0.5, 0.75, 1),
    min_child_weight = c(1,2) ,
    colsample_bytree = c(0.3,0.5)
  )
  
  xgb_trcontrol_1 = trainControl(
    method = "cv",
    number = 10,
    verboseIter = FALSE,
    returnData = FALSE,
    returnResamp = "all", # save losses across all models
    classProbs = TRUE, # set to TRUE for AUC to be computed
    summaryFunction = twoClassSummary,
    allowParallel = TRUE,
    
  )
  
  xgb_train_1 = train(
    x = as.matrix(grid_train %>% select(-Response)),
    y = factor(grid_train$Response),
    trControl = xgb_trcontrol_1,
    tuneGrid = xgb_grid_1,
    method = "xgbTree",
  )
  
  out= xgb_train_1$bestTune
  print(out)
  return(out)  
}

best_tune = xgb_gridsearch(train= train_2,ntrees = 1000)
```

### Result: [1] "Best hyperparameters combination:"



## XGB model

##**scale_pos_weight= 334399/46710 = 7.15** 


```{r}
knitr::kable(table(train$Response),
             col.names = c('Response','#'))
```

#### Launch Xgboost

```{r}
train_xgt <- function(train_data,test_data,best_tune)
{
  set.seed(123)
  print("XGB Train:")
  # One hot encoding
  dmy <- dummyVars(" ~ .", data = train_data)
  train_data <- data.frame(predict(dmy, newdata = train_data))
  
  # Select data & label
  train_data <- as.matrix(train_data)
  d_ata <- ncol(train_data)
  data_train <- train_data[,1:d_ata-1]
  label_train <- train_data[,d_ata]
  # Transform data
  data_train <- as.matrix(data_train)
  label_train <- as.numeric(label_train)
  #XGB Model
  dtrain = xgb.DMatrix(data_train,label=label_train)
  machine = xgboost(data= dtrain,  objective = "binary:logistic",
                    # paramaters
                    max_depth = best_tune$max_depth,
                    nrounds=1000,
                    colsample_bytree = best_tune$colsample_bytree,
                    gamma = best_tune$gamma,
                    min_child_weight = best_tune$min_child_weight,
                    eta = best_tune$eta, 
                    subsample = best_tune$subsample,
                    print_every_n = 200,
                    scale_pos_weight=7.15,
                    max_delta_step=1,
                    # others
                    verbose=1,
                    nthread = 4,
                    eval.metric = "auc")
  # Cross Validation
  print('XGB Cross Validation:')
  cv  <-  xgb.cv(data = dtrain, nround = 1000,
                 print_every_n = 200,
                 nthread = 4,
                 verbose = TRUE,
                 eval_metric= "auc",
                 nfold = 5, 
                 objective = "binary:logistic")
  # Encoding Test data & Transform
  dmy1 <- dummyVars(" ~ .", data = test_data)
  test_data <- data.frame(predict(dmy1, newdata = test_data))
  test_data <- as.matrix(test_data)
  
  # Predictions
  pred <- predict(machine, test_data)
  # Importance plot
  importance_matrix <- xgb.importance(colnames(data_train), model = machine)
  gg <- xgb.ggplot.importance(importance_matrix, rel_to_first = TRUE,xlab="Relative Importance")
  gg <- gg + ggplot2::ylab("Relative importance")
  
  out <- list("Features Importance:" = gg,
              Predictions=pred)
  
  return(out)  
}
train_xgt(train_data=train,test_data = test,best_tune=best_tune)
test_predictions <- train_xgt(train_data=train,
                              test_data = test,
                              best_tune=best_tune)$Predictions
```


## Test XGB on a balanced dataset

##**As a further test for our model, we will rebalance the original dataset and evaluate the model performance on a balanced dataset**


# Rebalance dataset

```{r}
ov_sampling <- ovun.sample(formula = Response ~. ,data=train,method='both',N=381109,seed=12)
train_3<- ov_sampling$data
paste("Length of train_3 dataset(Balanced dataset):",nrow(train_3))
knitr::kable(prop.table(table(train_3$Response)),
             col.names = c('Response','%'),)

```

#### Launch Xgboost

```{r}
train_xgt <- function(train_data,best_tune)
{
  set.seed(123)
  print("XGB is starting...")
  # One hot encoding
  dmy <- dummyVars(" ~ .", data = train_data)
  train_data <- data.frame(predict(dmy, newdata = train_data))
  
  # Select data and target
  train_data <- as.matrix(train_data)
  d_ata <- ncol(train_data)
  data_train <- train_data[,1:d_ata-1]
  label_train <- train_data[,d_ata]
  
  # Transform data  
  data_train <- as.matrix(data_train)
  label_train <- as.numeric(label_train)
  print("XGB Train:")
  # XGB Model  
  dtrain = xgb.DMatrix(data_train,label=label_train)
  machine = xgboost(data= dtrain,  objective = "binary:logistic",
                    # paramaters
                    max_depth = best_tune$max_depth,
                    nrounds=1000,
                    colsample_bytree = best_tune$colsample_bytree,
                    gamma = best_tune$gamma,
                    min_child_weight = best_tune$min_child_weight,
                    eta = best_tune$eta, 
                    subsample = best_tune$subsample,
                    print_every_n = 200,
                    max_delta_step=1,
                    # others
                    verbose=1,
                    nthread = 4,
                    eval.metric = "auc")
  print("XGB Cross Validation:")
  # Cross Validation
  cv  <-  xgb.cv(data = dtrain, nround = 1000, 
                 print_every_n= 200,
                 verbose = TRUE,
                 eval_metric= "auc",
                 nfold = 5, 
                 nthread = 4,
                 objective = "binary:logistic",
                 prediction=T)
  # Prediction
  # # predict on test label
  pred <- predict(machine, dtrain)
  prediction <- as.numeric(pred > 0.5)
  # Confusion Matrix
  cm = confusionMatrix(factor(prediction),factor(label_train),positive="1")
  
  cm_d <- as.data.frame(cm$table)
  # confusion matrix statistics as data.frame
  cm_st <-data.frame(cm$overall)
  # round the values
  cm_st$cm.overall <- round(cm_st$cm.overall,2)
  # here we also have the rounded percentage values
  cm_p <- as.data.frame(prop.table(cm$table))
  cm_d$Perc <- round(cm_p$Freq*100,2)
  
  # plotting the matrix
  cm_d_p <-  ggplot(data = cm_d, aes(x = Prediction , y =  Reference, fill = Freq))+
    geom_tile() +
    geom_text(aes(label = paste("",Freq,",",Perc,"%")), color = 'white', size = 4) +
    theme_light() +
    scale_fill_gradient(low = "#3C3838", high = "#338076") + 
    scale_x_discrete(position = "top") +
    guides(fill=FALSE) 
  
  # plotting the stats
  colnames(cm_st) <- ('Statistics')
  
  cm_st_p <-  tableGrob(cm_st)
  
  # all together
  conf_mat = grid.arrange(cm_d_p, cm_st_p,nrow = 1, ncol = 2, 
                          top=textGrob("Confusion Matrix and Statistics",gp=gpar(fontsize=25,font=1)))
  # Roc Curve
  roc.curve = roc(response = label_train,predictor = cv$pred,levels=c(0, 1)) 
  plot_roc = plot(roc.curve,main="ROC Curve",col="darkgreen")
  # Importance plot
  importance_matrix <- xgb.importance(colnames(data_train), model = machine)
  gg <- xgb.ggplot.importance(importance_matrix, rel_to_first = TRUE,xlab="Relative Importance")
  gg <- gg + ggplot2::ylab("Relative importance")
  
  
  # Out
  out <- list("Features Importance:" = gg,
              "Confusion Matrix"= conf_mat,
              "Roc Curve" = plot_roc)
  
  return(out)  
}

train_xgt(train_data=train_3,
          best_tune=best_tune)
```



# Submission (Checking Results) 

## Submit XGB predictions

```{r}
last_submission <- cbind(submission,predicted_response=round(test_predictions),real_predictions=test_predictions)
head(last_submission,5)
```

# Model Deployment

Once we've selected a model, we proceed to deploy it with a data pipeline into a production or production-like environment for final user acceptance. This prepares the model for seamless integration with the client's existing applications. We successfully deployed our model and presented it to Triks Insurance Ltd.

From our initial understanding of the business, we identified several ways in which the client aims to benefit from this project:

*Providing GOOL Auto with insights into the potential business Triks Insurance can generate from their existing client base.
*Equipping Triks Insurance staff with a tool to prioritize clients for targeted marketing campaigns.
*Enabling Triks Insurance to project future revenues.
*Offering real-time notifications for potential cross-selling opportunities.

Pipelines can be developed for each of these requirements, although they fall outside the scope of this project. Predictions can be generated in real-time or on a batch basis. We have deployed the model to predict cross-sells based on user-provided customer input, including gender, age, region code, policy sales channel, vehicle age, and vehicle damage. Additionally, another deployment facilitates data file uploads, allowing predictions for entire datasets.







# Conclusion




